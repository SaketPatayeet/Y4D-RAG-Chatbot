Was this it?
I am planning to do it like this
# -*- coding: utf-8 -*-
"""Hackathon.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-YItFuI8XMa9zKOjt3cA0KNPJRhHxD07
"""

"""from bs4 import BeautifulSoup
from urllib.parse import urljoin
import requests
from langchain.schema import Document

base_url = "https://www.y4d.ngo"
projects_url = f"{base_url}/projects"

# Get the projects page
html = requests.get(projects_url).text
soup = BeautifulSoup(html, "html.parser")

# Collect unique project links
project_links = set()
for a in soup.find_all("a", href=True):
    if "project_details" in a['href']:
        full_url = urljoin(base_url, a['href'])
        project_links.add(full_url)

project_links = list(project_links)
print("Unique project links:", project_links)

# Convert projects directly into Document objects
project_docs = []

for link in project_links:
    detail_html = requests.get(link).text
    detail_soup = BeautifulSoup(detail_html, "html.parser")

    title_tag = detail_soup.find("h3") or detail_soup.find("h1")
    title = title_tag.get_text(strip=True) if title_tag else "No title"

    desc_tag = detail_soup.find("div", class_="project-description")
    if desc_tag:
        description = desc_tag.get_text(separator="\n", strip=True)
    else:
        first_p = detail_soup.find("p")
        description = first_p.get_text(strip=True) if first_p else "No description"

    # Convert to Document with metadata
    doc = Document(
        page_content=description,
        metadata={"url": link, "title": title, "type": "project"}
    )
    project_docs.append(doc)

print(f"Total project Documents: {len(project_docs)}")

!pip install langchain-community

from bs4 import SoupStrainer
from langchain_community.document_loaders import WebBaseLoader
from langchain.schema import Document

about_url = f"{base_url}/who_are_we"
bs4_strainer = SoupStrainer(name=("h2", "p"))  # filter by tag, not class
loader = WebBaseLoader(
    web_paths=(about_url,),
    bs_kwargs={"parse_only": bs4_strainer},
)

web_docs = []
for doc in loader.load():
    web_docs.append(Document(
        page_content=doc.page_content,
        metadata={
            "source": about_url,
            "type": "webpage"
        }
    ))

for doc in web_docs:
    print(doc.page_content)

!pip install pypdf

import os
from langchain_community.document_loaders import PyPDFLoader
from langchain.schema import Document

pdf_folder = "PDFs"
pdf_files = [os.path.join(pdf_folder, f) for f in os.listdir(pdf_folder) if f.endswith(".pdf")]
all_docs = []  # master list to store everything

for pdf in pdf_files:
    loader = PyPDFLoader(pdf)
    pdf_pages = loader.load()      # docs = pages of this PDF only
    all_docs.extend(pdf_pages)     # add these pages to the master list
    for page in pdf_pages:
        all_docs.append(Document(
            page_content=page.page_content,
            metadata={
                "source": "https://www.y4d.ngo/newsletters",
                "file_name": pdf,
                "type": "pdf"
            }
        ))
print(f"Total pages: {len(all_docs)}")

all_rag_docs = web_docs + all_docs + project_docs
print(f"Total documents for RAG: {len(all_rag_docs)}")

print(f"Total characters: {len(all_rag_docs[0].page_content)}")

#SPLITTING INTO CHUNKS AND EMBEDDINGS

from langchain_text_splitters import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,  # chunk size (characters)
    chunk_overlap=200,  # chunk overlap (characters)
    add_start_index=True,  # track index in original document
)
all_splits = text_splitter.split_documents(all_rag_docs)

print(f"Split blog post into {len(all_splits)} sub-documents.")"""

#CHROMA AND RETRIVAL





from langchain_huggingface import HuggingFaceEmbeddings

embeddings_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-mpnet-base-v2")



"""import shutil

shutil.rmtree("./chroma_langchain_db", ignore_errors=True)"""

from langchain_chroma import Chroma


vectorstore = Chroma(
    collection_name="example_collection",
    embedding_function=embeddings_model,
    persist_directory=r"D:\Users\Desktop\Saket\ML\Hackathon\backend\chroma_db_backup"
)



#print("✅ Documents added to vectorstore!")



print(f"Vectors stored: {vectorstore._collection.count()}")

"""import shutil
shutil.make_archive("chroma_db_backup", 'zip', "./chroma_langchain_db")"""

retriever = vectorstore.as_retriever(search_type = "similarity",search_kwargs = {"k": 3})
retriever

#retriever.invoke('What does Y4D do?')

"""AUGMENTATION

"""
import os
from dotenv import load_dotenv
from huggingface_hub import InferenceClient
load_dotenv()
HF_TOKEN = os.getenv("HF_TOKEN")
client = InferenceClient(base_url="https://router.huggingface.co/v1",
                         api_key="HUGGING_FACE_API")
vision_client = InferenceClient(
    provider="novita",
    api_key="API_KEY",
)

from huggingface_hub import login
login(HF_TOKEN) 

from langchain_huggingface import HuggingFacePipeline
from transformers import pipeline
from transformers import AutoTokenizer, AutoModelForCausalLM

#model_name = "meta-llama/Llama-3.2-1B"
#tokenizer = AutoTokenizer.from_pretrained(model_name)
#model = AutoModelForCausalLM.from_pretrained(model_name, device_map={"": "cpu"})

"""pipe = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=512,
    eos_token_id=tokenizer.eos_token_id,
    pad_token_id=tokenizer.eos_token_id,
    temperature=0.3,
    do_sample=True, 
    repetition_penalty=1.2
)
llm = HuggingFacePipeline(pipeline=pipe)"""

class HFTextLLM:
    def __init__(self, model_name="openai/gpt-oss-120b:fireworks-ai"):
        self.model_name = model_name

    def generate(self, prompt: str, max_tokens=512, temperature=0.3):
        response = client.chat.completions.create(
            model=self.model_name,
            messages=[{"role": "user", "content": prompt}],
            max_tokens=max_tokens,
            temperature=temperature
        )
        # Access the generated text
        return response.choices[0].message["content"]

llm = HFTextLLM(model_name="openai/gpt-oss-120b:fireworks-ai")

from langchain.prompts import PromptTemplate

prompt = PromptTemplate(
    template="""
You are a helpful assistant for Y4D Foundation.
Y4D is an NGO which works on Child Upliftment, Youth Empowerment, and CSR Partnerships.

Answer questions using the provided context whenever possible.
Keep the answer short. 

If the user asks about donations, always include this fixed information:
- Phone: +91-XXXXXXXXXX
- Donation form: https://www.y4d.ngo/donate

- If the context does NOT contain the answer, still provide the fixed donation info for donation questions.
- For other questions with insufficient context, just say "I don't know."
Some questions may include information from an image, treat it as part of the user's query.
Answer fully and clearly, avoiding repetition.

Context: {context}
Question: {question}
Answer:""",
    input_variables=['context', 'question']
)

import base64
from PIL import Image
import io
class HFImageCaptionLLM:
    
    def __init__(self, model_name="zai-org/GLM-4.5V"):
        self.model_name = model_name

    def encode_image_to_b64(self, image_path: str, max_size=(512, 512)):
        img = Image.open(image_path)
        img.thumbnail(max_size)  # shrink while keeping aspect ratio
        buffer = io.BytesIO()
        img.save(buffer, format="JPEG", quality=80)  # compress
        buffer.seek(0)
        return base64.b64encode(buffer.read()).decode("utf-8")

    def describe_image(self, image_path: str):
            image_b64 = self.encode_image_to_b64(image_path)

        
            response = vision_client.chat.completions.create(
            model="zai-org/GLM-4.5V",
            messages=[
                {"role": "system", "content": "You are a helpful assistant that describes images."},
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": "Describe this image."},
                        {"type": "image", "image_url": {"url": f"data:image/jpeg;base64,{image_b64}"}}
                    ]
                }
            ],
            max_tokens=256
            )
        
        # Access text safely
            if isinstance(response, list) and "generated_text" in response[0]:
                return response[0]["generated_text"]
            return ""
        
image_llm = HFImageCaptionLLM()



#question          = "What are the main initiatives Y4D Foundation is running to empower youth?"
#retrieved_docs    = retriever.invoke(question)

#context_text = "\n\n".join(doc.page_content for doc in retrieved_docs)
#context_text

#retrieved_docs

#final_prompt = prompt.invoke({"context": context_text, "question": question})

#final_prompt

"""GENERATION"""

#final_prompt_str = str(final_prompt)
#answer = llm.invoke(final_prompt_str,stop=["\nQuestion:", "\nContext:"])
#print(answer)

#answer_only = answer.split("Answer:")[-1].strip()
#clean_answer = answer_only.replace("\n", " ").strip()
#print(clean_answer)
"""IMAGE HANDLING"""
import easyocr
from PIL import Image
from transformers import CLIPProcessor, CLIPModel

# OCR Reader
ocr_reader = easyocr.Reader(['en'], gpu=False)  # set gpu=True if available

# Image Captioning
from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer
from PIL import Image
import torch

# Load captioning model + processor
caption_model = VisionEncoderDecoderModel.from_pretrained("nlpconnect/vit-gpt2-image-captioning")
caption_processor = ViTImageProcessor.from_pretrained("nlpconnect/vit-gpt2-image-captioning")
caption_tokenizer = AutoTokenizer.from_pretrained("nlpconnect/vit-gpt2-image-captioning")

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
caption_model.to(device)

def process_image(image_path: str) -> str:
    """
    Given an image path, extract text using OCR and generate a caption.
    Returns combined string for LLM context.
    """
    # 1️⃣ OCR
    ocr_result = ocr_reader.readtext(image_path, detail=0)  # just text
    ocr_text = "\n".join(ocr_result) if ocr_result else ""

    # 2️⃣ Captioning
    image = Image.open(image_path).convert("RGB")
    pixel_values = caption_processor(images=image, return_tensors="pt").pixel_values.to(device)

    output_ids = caption_model.generate(pixel_values, max_length=16, num_beams=4)
    caption = caption_tokenizer.decode(output_ids[0], skip_special_tokens=True)

    # 3️⃣ Combine
    combined_text = f"Image description: {caption}\nText in image: {ocr_text}"
    return combined_text
 
from faster_whisper import WhisperModel

# pick a smaller model for hackathon speed
model = WhisperModel("tiny", device="cpu")

def process_audio(audio_path: str) -> str:
    segments, info = model.transcribe(audio_path)
    return " ".join([seg.text for seg in segments])

"""BUILDING A CHAIN"""

from langchain_core.runnables import RunnableParallel, RunnablePassthrough, RunnableLambda
from langchain_core.output_parsers import StrOutputParser
from deep_translator import GoogleTranslator, single_detection

def format_docs(retrieved_docs):
  context_text = "\n\n".join(doc.page_content for doc in retrieved_docs)
  #print(context_text)
  return context_text

parallel_chain = RunnableParallel({
    'context': retriever | RunnableLambda(format_docs),
    'question': RunnablePassthrough()
})

def clean_output(answer_text):
    lines = [line.strip() for line in answer_text.split("\n") if line.strip()]
    return "\n".join(lines)

def AnswerOnlyParser(text):
      if "Setting `pad_token_id`" in text:
        text = text.split("\n", 1)[1]
      if "Answer:" in text:
          return text.split("Answer:")[-1].strip()
      return text.strip()

parser = StrOutputParser()
llm_runnable = RunnableLambda(lambda prompt_text: llm.generate(str(prompt_text)))

main_chain = parallel_chain | prompt | llm_runnable | AnswerOnlyParser

#raw = main_chain.invoke('What did Y4D do for environment')
#clean = AnswerOnlyParser(raw)
#print(clean)
#print(main_chain.invoke('Tell me about the Green Day event at Amdocs'))

DONATION_INFO = "\nPhone: +91-XXXXXXXXXX\nDonation form: https://www.y4d.ngo/donate"

"""def answer_question(question: str, image_path: str = None) -> str:
    
    detected = single_detection(question,api_key="1a3e2a85b03d581b6f2991f2f5f33bbc")
    if detected != 'en':
        question = GoogleTranslator(source='auto', target='en').translate(question)

    if image_path:
        image_text = process_image(image_path)
        # Append image text to question
        question = question + "\n\nAdditional info from image:\n" + image_text
        
    # Check for donation keywords
    donation_keywords = ["donate", "donation", "support", "funds", "fundraising"]
    if any(word in question.lower() for word in donation_keywords):
        answer = f"You can donate by visiting the link or contacting us directly:{DONATION_INFO}"
        if detected != 'en':
            answer = GoogleTranslator(source='en', target=detected).translate(answer)
        print(answer)
        return answer

    # Otherwise, use your RAG chain
    answer = main_chain.invoke(question)
    if detected != 'en':
            answer = GoogleTranslator(source='en', target=detected).translate(answer)
    print(answer)
    return answer"""


def answer_question(question: str = "", image_path: str = None, audio_path: str = None) -> str:
    detected = single_detection(question, api_key="1a3e2a85b03d581b6f2991f2f5f33bbc")

    # Translate question if not English
    if detected != 'en':
        question = GoogleTranslator(source='auto', target='en').translate(question)

    # Add image info if provided
    if image_path:
        image_text = process_image(image_path)
        question += f"\n\nAdditional info from image:\n{image_text}"

    # Add audio info if provided
    audio_text = ""
    if audio_path:
        audio_text = process_audio(audio_path)
        question += f"\n\nAdditional info from audio:\n{audio_text}"

    # Donation shortcut
    donation_keywords = ["donate", "donation", "support", "funds", "fundraising"]
    if any(word in question.lower() for word in donation_keywords):
        answer = f"You can donate by visiting the link or contacting us directly:{DONATION_INFO}"
    else:
        # Use your RAG chain
        answer = main_chain.invoke(question)

    # Translate back if needed
    if detected != 'en':
        answer = GoogleTranslator(source='en', target=detected).translate(answer)

    print(answer)
    return answer
